{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Redes Neurais - Gabarito!\n",
    "## Grupo Turing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch logo](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• B√°sicos de Pytorch üî•\n",
    "\n",
    "Primeiro vamos ver alguns an√°logos entre **numpy** e **Pytorch**\n",
    "\n",
    "### Matrizes\n",
    " - Em Pytorch, matrizes (*arrays*) s√£o chamados de tensores.\n",
    " - Uma matriz $3\\times3$, por exemplo √© um tensor $3\\times3$\n",
    " - Podemos criar um array numpy com o m√©todo `np.array()`\n",
    " - Podemos pegar o tipo do array com `type()`\n",
    " - Podemos pegar o formato do *array* com `np.shape()`. Linha $\\times$ Coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "primeiro_array = np.array(array) # array 2x3\n",
    "print(f\"Array do tipo: {type(primeiro_array)}\")\n",
    "print(f\"Array de formato: {np.shape(primeiro_array)}\")\n",
    "print(primeiro_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criamos um tensor com o m√©todo `torch.Tensor()`\n",
    "- `tensor.type`: tipo do *array*, nesse caso um tensor\n",
    "- `tensor.shape`: formato do *array*. Linha $\\times$ Coluna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.Tensor(array)\n",
    "print(f\"Array do tipo: {tensor.type}\")\n",
    "print(f\"Array de formato: {tensor.shape}\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer a aloca√ß√£o de *arrays* de maneira an√°loga nas duas linguagens:\n",
    " - `np.ones()` = `torch.ones()`\n",
    " - `np.random.rand()` = `torch.rand()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy:\\n {np.ones((2,3))}\\n\")\n",
    "\n",
    "print(torch.ones((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy:\\n {np.random.rand(2,3)}\\n\")\n",
    "\n",
    "print(torch.rand(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo de numpy para torch e vice-versa\n",
    "\n",
    "Em muitos pontos **numpy** e **pytorch** s√£o bem parecidos em suas estruturas, e muitas das vezes podemos utilizar os dois em conjunto. Assim normalmente convertemos resultados de redes neurais - que s√£o tensores - para **arrays** de **numpy**.\n",
    "\n",
    "Os m√©todos para fazer a convers√£o entre tensores e arrays numpy:\n",
    " - `torch.from_numpy()`: de um array numpy para um tensor\n",
    " - `tensor.numpy()`: de um tensor para um array numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.rand(2,2)\n",
    "print(f\"{type(array)} \\n {array} \\n\")\n",
    "\n",
    "de_numpy_para_tensor = torch.from_numpy(array)\n",
    "print(f\"{de_numpy_para_tensor} \\n\")\n",
    "\n",
    "tensor = de_numpy_para_tensor\n",
    "de_tensor_para_numpy = tensor.numpy()\n",
    "print(f\"{type(de_tensor_para_numpy)} \\n {de_tensor_para_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando fazemos estas convers√µes tamb√©m podemos fazer um *typecast* (mudan√ßa do tipo) das vari√°veis, isso pode ser √∫til j√° que o Pytorch faz uma s√©rie de computa√ß√µes de baixo n√≠vel, o qual o tipo primitivo das vari√°veis precisa ser bem especificado e definido, para isso podemos usar o m√©todo `tensor.type(torch.TipoDeTensor)`, alguns tipo de tensores nativos do Pytorch s√£o:\n",
    "  - `torch.FloatTensor` - pontos flutuantes de 32-bits\n",
    "  - `torch.DoubleTensor` - pontos flutuantes de 64-bits\n",
    "  - `torch.IntTensor` - n√∫meros inteiros de 32-bits\n",
    "  - `torch.LongTensor` - n√∫meros inteiros de 64-bits\n",
    "√â muito comum encontrarmos *bugs* causados pela utiliza√ß√£o errada de algum tipo primitivo, voc√™ pode ler sobre todos eles na [documenta√ß√£o do Pytorch](https://pytorch.org/docs/stable/tensors.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1,10],[2,20]])\n",
    "\n",
    "# Transformar em um tensor de Floats:\n",
    "tensor_float = torch.from_numpy(array).type(torch.FloatTensor)\n",
    "print(f\"{type(tensor_float)} \\n {tensor_float}\\n\")\n",
    "\n",
    "# Transformar em um tensor de Longs:\n",
    "tensor_long = torch.from_numpy(array).type(torch.LongTensor)\n",
    "print(f\"{type(tensor_long)} \\n {tensor_long}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matem√°tica b√°sica com Pytorch\n",
    "*considere a e b dois tensores*\n",
    "\n",
    "- Redefinir o tamanho: `view()`\n",
    "- Adi√ß√£o: `torch.add(a,b)` = a + b\n",
    "- Subtra√ß√£o: `torch.sub(a,b)` = a - b\n",
    "- Multiplica√ß√£o elemento-a-elemento = `torch.mul(a,b)` = a * b\n",
    "- Divis√£o elemento-a-elemento = `torch.div(a,b)` = a / b\n",
    "- M√©dia: `a.mean()`\n",
    "- Desvio Padr√£o (Standart Deviantion - std): `a.std()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(3,3)\n",
    "print(\"\\n\", tensor, \"\\n\")\n",
    "\n",
    "print(f\"{tensor.view(9).shape}: {tensor.view(9)} \\n\")\n",
    "\n",
    "print(f\"Adi√ß√£o: \\n{torch.add(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Subtra√ß√£o: \\n{torch.sub(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Multiplica√ß√£o elemento-a-elemento: \\n{torch.mul(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Divis√£o elemento-a-elemento: \\n{torch.div(tensor, tensor)} \\n\")\n",
    "\n",
    "tensor = torch.Tensor([1,2,3,4,5])\n",
    "print(f\"M√©dia: {tensor.mean()} \\n\")\n",
    "\n",
    "print(f\"Desvio padr√£o: {tensor.std()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vari√°veis\n",
    "\n",
    "- Acumulam os gradientes\n",
    "- Na rede neural utilizaremos pytorch. Como explicamos anteriormente nas, redes neurais os gradientes s√£o calculados na *backpropagation*.\n",
    "- A diferen√ßa entre vari√°veis e tensores √© a de que vari√°veis acumulam os gradientes\n",
    "- Tamb√©m podemos fazer opera√ß√µes matem√°ticas com vari√°veis\n",
    "- Dessa maneira, se queremos fazer a *backpropagation* precisamos de vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "var = Variable(torch.ones(3), requires_grad = True)\n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver um exemplo de como as Vari√°veis s√£o utilizadas em uma *backpropagation*, com duas fun√ß√£o $f(y) = \\sum y$, $y(x) = x^2$ e $x = (3,5)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [3,5]\n",
    "tensor = torch.Tensor(array)\n",
    "x = Variable(tensor, requires_grad = True)\n",
    "y = x**2\n",
    "print(f\" x = {y}\")\n",
    "\n",
    "f = sum(y)\n",
    "print(f\" f =  {f}\")\n",
    "\n",
    "f.backward() # Realiza as derivadas parciais\n",
    "\n",
    "print(f\"Gradientes: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos explicar passo a passo quais foram as opera√ß√µes feitas pelo Pytorch:\n",
    "- Primeiro ele recebe os elementos do tensor e faz a primeira opera√ß√£o com eles $y_1 = 3^2 = 9$ e $y_2 = 5^2 = 25$\n",
    "- Agora ele soma o tensor, retornando assim um √∫nico valor escalar: $\\sum_i y_i = y_1 + y_2 = 9 + 25 = 34$\n",
    "- O gradiente √© a derivada parcial de cada elemento, ou seja o gradiente \"1\" √© a derivada relativa √† $y_1$ e o gradiente \"2\" √© relativo √† $y_2$ \n",
    "- derivada relativa √† $y_1$ √© $\\frac{\\partial}{\\partial y_1}(3^2) = 2*3 = 6$\n",
    "- derivada relativa √† $y_2$ √© $\\frac{\\partial}{\\partial y_2}(5^2) = 2*5 = 10$\n",
    "- Assim ficamos com os gradientes $(6, 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Implementando uma rede neural üß†\n",
    "### Conhecendo e preparando nossos dados \n",
    "\n",
    "**[Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist)** √© uma cole√ß√£o de diversas pe√ßas de roupas retiradas do servi√ßo de *e-commerce* Zalando, ele consiste de cerca de 60.000 entradas de treino de 10.000 de teste. Cada entrada √© uma imagem de 28x28 pixels em escala cinza. As pe√ßas de roupa est√£o classificadas da seguinte maneira:\n",
    "  - 0 *T-shirt*\n",
    "  - 1 *Trouser*\n",
    "  - 2 *Pullover*\n",
    "  - 3 *Dress*\n",
    "  - 4 *Coat*\n",
    "  - 5 *Sandal*\n",
    "  - 6 *Shirt*\n",
    "  - 7 *Sneaker*\n",
    "  - 8 *Bag*\n",
    "  - 9 *Ankle boot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_inicial = pd.read_csv(\"https://github.com/GrupoTuring/Workshop-de-redes-neurais/raw/main/fashion-mnist_train.zip\")\n",
    "df_inicial.head() # Como cada coluna representa o valor de cada pixel, a tabela dos dados n√£o √© muito \"emocionante\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validos = 10000\n",
    "n_treino = len(df_inicial) - n_validos\n",
    "print(f\"N√∫mero de entradas de treino: {n_treino}\\n\"\n",
    "      f\"N√∫mero de entradas de valida√ß√£o: {n_validos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos essa fun√ß√£o para dividir entre dados de treino e valida√ß√£o\n",
    "def divide_valores(a,n):\n",
    "     return a[:n].copy(), a[n:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dataset entre dados x e labels y\n",
    "y, x = df_inicial[\"label\"].values, df_inicial.loc[:, df_inicial.columns != \"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treino, x_valido = divide_valores(x, n_treino)\n",
    "y_treino, y_valido = divide_valores(y, n_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Formato do x de treino: {x_treino.shape}\\n\"\n",
    "      f\"Formato do x de valida√ß√£o: {x_valido.shape}\\n\"\n",
    "      f\"Formato do y de treino: {y_treino.shape}\\n\"\n",
    "      f\"Formato do y de valida√ß√£o: {y_valido.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma etapa comum de pr√© processamento de dados em aprendizado por m√°quina √© centralizar padronizar nosso *dataset*, o que isso basicamente significa √© que iremos subtrair a m√©dia de todo o *dataset* e dividi-lo pelo seu desvio padr√£o. Esse processo ajuda a agilizar o processo de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = x_treino.mean()\n",
    "desvio_padrao = x_treino.std()\n",
    "\n",
    "x_treino = (x_treino-media)/desvio_padrao\n",
    "print(f\"M√©dia antes do pr√© processamento: {media:.2f}\\n\"\n",
    "      f\"Desvio padr√£o antes do pr√© processamente: {desvio_padrao:.2f}\\n\"\n",
    "      f\"M√©dia depois do pr√© processamento: {x_treino.mean():.2f}\\n\"\n",
    "      f\"Desvio padr√£o depois do pr√© processamento: {x_treino.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo deve ser feito com a valida√ß√£o\n",
    "\n",
    "x_valido = (x_valido-media)/desvio_padrao\n",
    "print(f\"M√©dia p√≥s processamento: {x_valido.mean():.2f}\\n\"\n",
    "      f\"Desvio padr√£o p√≥s processamento: {x_valido.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar algumas das imagens de nosso *dataset*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Essa fun√ß√£o vai nos ajudar a visualizar as imagens\n",
    "def mostrar(img, title=None):\n",
    "    labels = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    if title is not None: plt.title(labels[int(title)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_imgs = np.reshape(x_valido, (-1, 28, 28))\n",
    "\n",
    "index = 100\n",
    "mostrar(x_imgs[index], y_valido[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptando os dados para o Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforme os respectivos datasets de numpy para tensores torch, \n",
    "# o tensor x_treino_torch deve ser do tipo torch.FloatTensor\n",
    "# Dica, lembre-se da se√ß√£o \"Convertendo de numpy para torch e vice-versa\"\n",
    "x_treino_torch = torch.from_numpy(x_treino).type(torch.FloatTensor)\n",
    "y_treino_torch = torch.from_numpy(y_treino)\n",
    "\n",
    "x_valido_torch = torch.from_numpy(x_valido).type(torch.FloatTensor)\n",
    "y_valido_torch = torch.from_numpy(y_valido)\n",
    "\n",
    "tamanho_batch = 100\n",
    "n_iters = 10000\n",
    "\n",
    "n_epochs = int((n_iters / len(y_treino)) * tamanho_batch)\n",
    "\n",
    "# Transformamos em um dataset de tensores\n",
    "treino = torch.utils.data.TensorDataset(x_treino_torch, y_treino_torch)\n",
    "validacao = torch.utils.data.TensorDataset(x_valido_torch, y_valido_torch)\n",
    "\n",
    "# Preparamos o dataset para ser iterado pela rede\n",
    "treino_loader = DataLoader(treino, batch_size=tamanho_batch, shuffle=True)\n",
    "validacao_loader = DataLoader(validacao, batch_size=tamanho_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para que serviu o TensorDataset e o DataLoader?\n",
    "\n",
    "Basicamente, o `TensorDataset` √© uma abstra√ß√£o que prepara nossos dados para serem tratados como um dataset de tensores pelo o Pytorch, j√° o `DataLoader` √© um iterador que n√≥s ajuda a fazer alguns tratamentos bem √∫til em nosso dataset, como o batching e shuffle dele, em casos mais avan√ßados ele tamb√©m te possibilita trabalhar em como ser√° feito o processamento desses dados pelo processador.\n",
    "\n",
    "A estrutura do nosso loader pode ser resumida assim:\n",
    "```\n",
    "- üì¶ 500 batches de pares de tensores\n",
    "  |- üç± 100 Tensores de Imagens e üóÉÔ∏è 100 labels respectivas\n",
    "     |- üñºÔ∏è 1 imagem como tensor   |Ô∏è-Ô∏èÔ∏è üè∑Ô∏è 1 label da imagem\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limite = 1\n",
    "contador = 0\n",
    "for i in treino_loader:\n",
    "    if contador < limite:\n",
    "         print(f\"Formato de cada batch: {len(i)}, {len(i[0])} \\n\"\n",
    "               f\"{i} \\n\"\n",
    "               f\"-------------------------------------------- \\n\")\n",
    "    else: break\n",
    "    contador += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treino_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podemos observar que nosso treino_loader √© composto de 500 pares de tensores, vamos olhar melhor cada um desses tensores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "limite = 2\n",
    "contador = 0\n",
    "for i in treino_loader:\n",
    "    if contador < limite:\n",
    "        print(f\"Tensor de imagens: {i[0]}\\n\"\n",
    "            f\"Tamanho do Tensor: {len(i[0])}\\n\"\n",
    "            f\"Formato do Tensor: {i[0].shape}\\n\"\n",
    "            f\"Imagem tensor: {i[0][0]}\\n\"\n",
    "            f\"Tamanho da imagem tensor: {len(i[0][0])}\\n\"\n",
    "            f\"------------------------------------------\")\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui pode-se notar que o primeiro tensor do par √© um tensor em que cada elemento √© um batch de 100 imagens-tensor. Vamos olhar o segundo elemento do par:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "limite = 2\n",
    "contador = 0\n",
    "for i in treino_loader:\n",
    "    if contador < limite:\n",
    "        print(f\"Tensor de labels: {i[1]}\\n\"\n",
    "            f\"Tamanho do Tensor: {len(i[1])}\\n\"\n",
    "            f\"Label: {i[1][0]}\\n\"\n",
    "            f\"------------------------------------------\")\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui pode-se notar que o segundo tensor do par √© um tensor em que cada elemento √© um batch de 100 classifica√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo para uma Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por que adicionar mais camadas?** Podemos pensar em cada camada como um n√≠vel de abstra√ß√£o, no nosso exemplo podemos pensar na camada de entrada como a a imagem, o primeiro *hidden layer* como algo que detecta bordas na imagem e o segundo *hidden layer* como  algo que detecta formas e padr√µes, e o nossa camada de sa√≠da, o estilo da roupa. Basicamente quando aumentamos o n√∫mero de camadas aumentamos o n√∫mero par√¢metros no nosso modelo, assim podendo treinar fun√ß√µes mais complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A fun√ß√£o de ativa√ß√£o retificadora linear** (*Rectified Linear Activation Function*) - ReL \n",
    "\n",
    "Para conseguirmos passar um sinal para a pr√≥xima camada, necessitamos de fun√ß√µes de ativa√ß√£o. Duas fun√ß√µes comuns s√£o as [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) e [tangente hiperb√≥lica](https://mathworld.wolfram.com/HyperbolicTangent.html), ambas fun√ß√µes n√£o lineares, uma propriedade que ajuda nosso modelo a compreender fun√ß√µes mais complexas. Por√©m, como elas s√£o fun√ß√µes com limites bem estabelecidos, elas acabam \"saturando\" suas sa√≠das, sendo sens√≠veis apenas para seus valores intermedi√°rios. A solu√ß√£o √© utilizar a fun√ß√£o de ativa√ß√£o retificadora linear (ReL) nos *hidden layers*. Dizemos que um n√≥ (ou neur√¥nio) com essa fun√ß√£o de ativa√ß√£o √© uma unidade de ativa√ß√£o retificadora linear (ReLU)\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "    x & \\text{if } x > 0, \\\\\n",
    "    0 & \\text{caso contr√°rio}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "source": [
    "![Fun√ß√µes de Ativa√ß√£o](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1200%2F1*ZafDv3VUm60Eh10OeJu1vw.png&f=1&nofb=1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O que √© a fun√ß√£o nn.Sequential() do torch?** Ele √© uma esp√©cie de container que cria nosso modelo, onde cada camada √© colocada na ordem que definimos dentro da fun√ß√£o. N√£o √© necess√°rio sempre utilizar essa fun√ß√£o para criar um modelo em torch, mas ela simplifica um pouco nosso trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedeNeural(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RedeNeural, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(        # Inicie a fun√ß√£o nn.Sequential()\n",
    "            nn.Linear(input_dim, 128),      # Primeira camada com nn.Linear(input_dim, 32)\n",
    "            nn.ReLU(),                      # Colocar a unidade de ativa√ß√£o nn.ReLU()\n",
    "            nn.Linear(128, 128),            # Segunda camada com nn.Linear(32, 32)\n",
    "            nn.ReLU(),                      # Colcoar a unidade de ativa√ß√£o ReLU\n",
    "            nn.Linear(128, output_dim),     # √öltima camada nn.Linear(32, output_dim)\n",
    "            nn.LogSoftmax()                 # √öltima camada de ativa√ß√£o nn.LogSoftmax\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)                  # Fa√ßa a forward propagation como na regress√£o log√≠stica\n",
    "        return y\n",
    "\n",
    "# Defina as dimens√µes de entrada e sa√≠da\n",
    "input_dim = 28*28 \n",
    "output_dim = 10\n",
    "\n",
    "# Fun√ß√£o de perda (loss)\n",
    "erro_nn = nn.NLLLoss()\n",
    "\n",
    "# Incialize o modelo\n",
    "modelo_nn = RedeNeural(input_dim, output_dim)\n",
    "\n",
    "# Otimizador SGD\n",
    "learning_rate = 0.02\n",
    "otimizador_nn = torch.optim.SGD(modelo_nn.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contagem = 0\n",
    "lista_loss = []\n",
    "lista_iteracao = []\n",
    "for epoch in range(n_epochs):\n",
    "    for imagens, classificacao in treino_loader:\n",
    "        \n",
    "        # Utilizando a fun√ß√£o Variable() crie as seguintes vari√°veis:\n",
    "        treino = Variable(imagens.view(100, 28*28)) # Crie uma vari√°vel com as imagens, por√©m, mude a forma do tensor\n",
    "                                                    # para ([100,28*28]) com o m√©todo .view(), ao inv√©s de 100, vc\n",
    "                                                    # pode escrever `-1`, que o pytorch pega a dimens√£o para vc.\n",
    "        \n",
    "        validacao = Variable(classificacao)     # Cire uma vari√°vel com a classificacao\n",
    "\n",
    "        # Limpamos os gradientes\n",
    "        otimizador_nn.zero_grad()\n",
    "\n",
    "        # Utilize o m√©todo .forward() do modelo utilizando a vari√°vel de treino\n",
    "        # para realizarmos a forward propagation\n",
    "        outputs = modelo_nn(treino) \n",
    "\n",
    "        # Utilize a fun√ß√£o de perda erro() com nosso output e a vari√°vel de verifica√ß√£o\n",
    "        loss_nn = erro_nn(outputs, validacao)\n",
    "\n",
    "        # Realizamos a backward propagation\n",
    "        loss_nn.backward()\n",
    "\n",
    "        # Atualiza os par√¢metros\n",
    "        otimizador_nn.step() \n",
    "\n",
    "        contagem += 1\n",
    "\n",
    "        # Predi√ß√µes\n",
    "        if contagem % 50 == 0:\n",
    "            # Calculamos a acur√°cia\n",
    "            corretos = 0\n",
    "            total = 0\n",
    "\n",
    "            for imagens, classificacao in treino_loader:\n",
    "                \n",
    "                # Crie uma vari√°vel com as imagens da mesma maneira como na vari√°vel de treino\n",
    "                teste = Variable(imagens.view(-1, 28*28))\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = modelo_nn(teste)\n",
    "                \n",
    "                # Recebe as predi√ß√µes do valor m√°ximo\n",
    "                predito =  torch.max(outputs.data, 1)[1]\n",
    "\n",
    "                # N√∫mero total de classifica√ß√µes\n",
    "                total += len(classificacao)\n",
    "\n",
    "                # N√∫mero de predi√ß√µes corretas\n",
    "                corretos += (predito == classificacao).sum()\n",
    "\n",
    "            acuracia = 100 * corretos / float(total)\n",
    "\n",
    "            # Armazena a loss e itera√ß√£o\n",
    "            lista_loss.append(loss_nn.data)\n",
    "            lista_iteracao.append(contagem)\n",
    "\n",
    "        if contagem % 500 == 0:\n",
    "            # Printa a loss\n",
    "            print(f\"Itera√ß√£o: {contagem} | Loss: {loss_nn.data} | Acur√°cia: {acuracia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,6))\n",
    "plt.plot(lista_iteracao,lista_loss)\n",
    "plt.xlabel(\"N√∫mero de Itera√ß√µes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Rede Neural: loss vs N√∫mero de Itera√ß√µes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè† Para casa! üè†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cios de PyTorch B√°sico\n",
    "\n",
    "Coplete √†s c√©lulas de c√≥digo abaixo no campo indicado por \"...\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um tensor com base no *array* dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[10,100,1000], [20,200,2000]]\n",
    "tensor = torch.Tensor(array)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um tensor de formato $(5,3)$ no qual todos os elementos s√£o o n√∫mero 1, depois utilize o m√©todo `.shape` para verificar seu formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_de_uns = torch.ones(5,3)\n",
    "formato_do_tensor = tensor_de_uns.shape\n",
    "\n",
    "print(f\"Tensor: \\n {tensor_de_uns}\")\n",
    "print(f\"Formato: {formato_do_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converta o *array* numpy para um tensor de Pytorch, depois transforme o tensor em um *array* numpy novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1,1,2,3], [5,8,13,21]])\n",
    "\n",
    "de_numpy_para_tensor = torch.from_numpy(array)\n",
    "print(f\"{de_numpy_para_tensor} \\n √â um tensor? {isinstance(de_numpy_para_tensor, torch.Tensor)}\")\n",
    "\n",
    "de_tensor_para_numpy = de_numpy_para_tensor.numpy()\n",
    "print(f\"{de_tensor_para_numpy} \\n √â um array numpy? {isinstance(de_tensor_para_numpy, np.ndarray)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a c√©lula abaixo com as opera√ß√µes indicadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = torch.Tensor([[5,8],[5,4]])\n",
    "tensor_b = torch.Tensor([[10,16],[10,8]])\n",
    "\n",
    "soma = torch.add(tensor_a,tensor_b) # a+b\n",
    "subtracao = torch.sub(tensor_b, tensor_a) # b-a\n",
    "mul = torch.mul(tensor_a, tensor_b) # a*b\n",
    "div = torch.div(tensor_b, tensor_a) # b/a\n",
    "media = tensor_a.mean() # media de a\n",
    "std = tensor_b.std() # desvio padr√£o de b\n",
    "\n",
    "print(f\"Soma:\\n {soma} \\n\"\n",
    "      f\"Subtra√ß√£o:\\n {subtracao} \\n\"\n",
    "      f\"Multiplica√ß√£o:\\n {mul} \\n\"\n",
    "      f\"Divis√£o:\\n {div} \\n\"\n",
    "      f\"M√©dia: {media} \\n\"\n",
    "      f\"Desvio Padr√£o: {std} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie uma **Var√≠avel** do Pytorch com o tensor definido. Depois defina as equa√ß√µes $y = log_e(x)$ e $f(y) = 2*media(y)$. Para ent√£o aplicar a *backpropagation* em $f(x)$ e calcular seus gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [4,5]\n",
    "tensor = torch.Tensor(array)\n",
    "\n",
    "x = Variable(tensor, requires_grad=True)\n",
    "print(f\" x = {x}\")\n",
    "\n",
    "y = torch.log(x) # Dica: use o operador torch.log()\n",
    "print(f\" y = {y}\")\n",
    "\n",
    "f = 2 * y.mean()\n",
    "print(f\" f = {f}\")\n",
    "\n",
    "# Escreva aqui a backpropagation de f\n",
    "f.backward()\n",
    "\n",
    "if isinstance(x, torch.Tensor): print(f\"Gradientes: {x.grad}\")\n",
    "else: print(\"Complete o exerƒáicio!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma regress√£o log√≠stica\n",
    "\n",
    "Dizemos que uma regress√£o linear √© basicamente uma maneira de visualizar nossos dados em uma \"linha\" e que a partir dessa linha podemos fazer algumas predi√ß√µes sobre dados futuros.\n",
    "\n",
    "Por√©m, regress√µes lineares n√£o s√£o muito boas com classifica√ß√µes, para isso utilizaremos uma regress√£o log√≠stica.\n",
    "\n",
    "Uma regress√£o log√≠stica √© uma regress√£o linear que utiliza a fun√ß√£o **softmax** como uma **fun√ß√£o de ativa√ß√£o**:\n",
    "$$\n",
    "a_i = \\frac{e^{x+i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Obs:** O que a fun√ß√£o **softmax** basicamente faz √© receber um vetor (lista) de valores num√©ricos e os transforma em valores probabil√≠sticos. Em outras palavras, quanto maior for o valor da prefer√™ncia daquela pat√¢metro, depois que essa lista de valores passar pela fun√ß√£o Softmax, maior ser√° sua probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dizemos que uma regress√£o log√≠stica √© basicamente uma rede neural bem simples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama de uma regress√£o log√≠stica](https://i.imgur.com/IfybNNw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressaoLogistica(nn.Module):\n",
    "    def __init__ (self, input_dim, output_dim):\n",
    "        super(RegressaoLogistica, self).__init__()  # J√° que ser√° \"filho\" de nn.Module, temos que iniciar seu \"pai\"\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim), # Parte linear\n",
    "            nn.LogSoftmax()                   # A parte log√≠stica\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "erro = nn.NLLLoss() # Fun√ß√£o de perda (loss)\n",
    "\n",
    "input_dim = 28*28 # Dica, olhe na especifica√ß√£o do problema\n",
    "output_dim = 10\n",
    "\n",
    "# Inicialize a Regress√£o Log√≠stica com as dimens√µes de input e output estabelecidas\n",
    "modelo = RegressaoLogistica(input_dim, output_dim) \n",
    "\n",
    "# Aqui escolhemos nosso otimizador, que nesse caso ser√° o Stochastic gradient descent, \n",
    "# que estar√° otimizando os par√¢metros de nossa regrss√£o linear.\n",
    "taxa_aprendizado = 0.001\n",
    "otimizador = torch.optim.SGD(modelo.parameters(), lr=taxa_aprendizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos agora treinar nosso modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contagem = 0\n",
    "lista_loss = []\n",
    "lista_iteracao = []\n",
    "for epoch in range(n_epochs):\n",
    "    for imagens, classificacao in treino_loader:\n",
    "        \n",
    "        # Utilizando a fun√ß√£o Variable() crie as seguintes vari√°veis:\n",
    "        treino = Variable(imagens.view(-1, 28*28)) # Crie uma vari√°vel com as imagens, por√©m, mude a forma do tensor\n",
    "                                                   # para (-1,28*28) com o m√©todo .view()\n",
    "        \n",
    "        validacao = Variable(classificacao)     # Cire uma vari√°vel com a classificacao\n",
    "\n",
    "        # Limpamos os gradientes\n",
    "        otimizador.zero_grad()\n",
    "\n",
    "        # Utilize o m√©todo .forward() do modelo utilizando a vari√°vel de treino\n",
    "        # para realizarmos a forward propagation\n",
    "        outputs = modelo(treino) \n",
    "\n",
    "        # Utilize a fun√ß√£o de perda erro() com nosso output e a vari√°vel de verifica√ß√£o\n",
    "        loss = erro(outputs, validacao)\n",
    "\n",
    "        # Realizamos a backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Atualiza os par√¢metros\n",
    "        otimizador.step() \n",
    "\n",
    "        contagem += 1\n",
    "\n",
    "        # Predi√ß√µes\n",
    "        if contagem % 50 == 0:\n",
    "            # Calculamos a acur√°cia\n",
    "            corretos = 0\n",
    "            total = 0\n",
    "\n",
    "            for imagens, classificacao in treino_loader:\n",
    "                \n",
    "                # Crie uma vari√°vel com as imagens da mesma maneira como na vari√°vel de treino\n",
    "                teste = Variable(imagens.view(-1, 28*28))\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = modelo(teste)\n",
    "                \n",
    "                # Recebe as predi√ß√µes do valor m√°ximo\n",
    "                predito =  torch.max(outputs.data, 1)[1]\n",
    "\n",
    "                # N√∫mero total de classifica√ß√µes\n",
    "                total += len(classificacao)\n",
    "\n",
    "                # N√∫mero de predi√ß√µes corretas\n",
    "                corretos += (predito == classificacao).sum()\n",
    "\n",
    "            acuracia = 100 * corretos / float(total)\n",
    "\n",
    "            # Armazena a loss e itera√ß√£o\n",
    "            lista_loss.append(loss.data)\n",
    "            lista_iteracao.append(contagem)\n",
    "\n",
    "        if contagem % 500 == 0:\n",
    "            # Printa a loss\n",
    "            print(f\"Itera√ß√£o: {contagem} | Loss: {loss.data} | Acur√°cia: {acuracia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,6))\n",
    "plt.plot(lista_iteracao,lista_loss)\n",
    "plt.xlabel(\"N√∫mero de Itera√ß√µes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Regress√£o Log√≠stica: Loss vs N√∫mero de Itera√ß√µes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ficou com alguma d√∫vida? Entre no nosso [Discord](https://discord.gg/26RGmBS) e tentaremos te ajudar!"
   ]
  },
  {
   "source": [
    "## E agora? Ficou querendo mais? Voc√™ pode\n",
    "\n",
    "  - üëÄ Checar essa [playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) do canal 3Blue 1Brown sobre o assunto e rever seus conhecimento.\n",
    "  - üìö Olhar a documenta√ß√£o do [PyTorch](https://pytorch.org/docs/stable/index.html).\n",
    "  - üì≥ Entrar no nosso [Discord](https://discord.gg/26RGmBS), uma comunidade crescente de pessoas interessadas em IA!\n",
    "  - üèãÔ∏èÔ∏è Ô∏èExplorar e tentar aplicar seu conhecimento em outros datasets, voc·∫Ω pode tentar procurar por projetos no [Kaggle](https://www.kaggle.com/). Recomendamos o [MNIST original](https://www.kaggle.com/c/digit-recognizer) e o [Sign Language MNIST](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a5cd74ba85a3b6a037c59ac3f3634fcdd9437555c9fe253dd51f04000fcd493e"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}